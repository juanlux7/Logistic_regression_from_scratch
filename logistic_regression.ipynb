{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression for binary classification problems from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in this notebook, I create a simple logistic regression classifier from scratch, just to show the mathematics behind this Supervised learning algorithm. Of course you can improve this code, by adding OOP, defining methods, avoiding hardcoded values in order to adapt this to a wider range of situations (e.g. no matter the dimensions of the data) etc... But the main purpose of this notebook is to show the process of creating a basic logistic regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, I load the data from sklearn and do some checking stuff, like the name of the keys, the shape of the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['data'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This step is very important. I make use of Pandas to create a dataframe only with the desired keys, like the raw data, the target of every row and the features names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_df = pd.DataFrame(np.c_[data['data'], data['target']], columns=np.append(data['feature_names'], 'target'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in this problem we try to identity which type of flower we have depending on features like the sepal length, sepal width, petal length and petal width. So this data only have 4 features and 1 output, which represents the class the flower belongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0     0.0  \n",
       "1     0.0  \n",
       "2     0.0  \n",
       "3     0.0  \n",
       "4     0.0  "
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "petal_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in the next step I do some slicing to get rid of the last 50 rows. Originally, This is a multiclass problem (with 3 different classes) and in order to convert this into a binary classification problem I got rid of one of the classes (class 2, from 101-150) - so now I have only two possible classes and a binary problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_df = petal_df[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After that I define X and y, in this case X corresponds to all the independent variables while y is the dependent variable (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = petal_df.iloc[:, :-1].values\n",
    "y = petal_df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### then I divide the dataset into training and test sets with the help of sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One important step before going any further is to declare all the features individually so we can make use of them in the following equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = X_train[:,0]\n",
    "x2 = X_train[:,1]\n",
    "x3 = X_train[:,2]\n",
    "x4 = X_train[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x1.reshape(90,1)\n",
    "x2 = x2.reshape(90,1)\n",
    "x3 = x3.reshape(90,1)\n",
    "x4 = x4.reshape(90,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(90,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When performing logistic regression, is a required step to define a sigmoid function in order to squeeze all the values beetwen 0 and 1. If we wouldn't do this, then we might get values above 1 and below 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bellow here we have some terms like m, which is the length of the train dataset, the learning rate, a required parameter to reduce our cost function by sustracting tiny steps to the cost function and partial derivatives, and finally all the theta parameters depending on the dimension of our data (in this case we have 4 dimensions corresponding to 4 features), each of them initialized with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(X_train)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "theta0 = np.zeros((m,1))\n",
    "theta1 = np.zeros((m,1))\n",
    "theta2 = np.zeros((m,1))\n",
    "theta3 = np.zeros((m,1))\n",
    "theta4 = np.zeros((m,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellow this text, I have appied Batch gradient descent (GD with all the data available): first I declare the number of iterations (epochs) this loop is going to take (10000) in order to reduce our cost function. I am going to explain all the steps that this proccess takes to converge in \n",
    "\n",
    "### - 1st step: I define the linear equation for this problem, which is: theta0 + theta(i) * feature(i)... you can find this equation in the internet easily\n",
    "\n",
    "### - 2nd step: we pass that value though a sigmoid function to get an output between 0 and 1 and work with consistent and probability values\n",
    "\n",
    "### - 3rd Step: We define our cost function by applying the cost function for logistic regression. This optimized equation was obtained from professor Andrew NG, Stanford University, and test the hypothesis (our prediction) with the true label (y_train), the dot product between matrices.\n",
    "\n",
    "### - 4th Step: keep in mind in the first iterations, this cost function will be huge, so we need to tweak some parameters to reduce this cost error. And because we can't change our data, the only possible way to reduce the error is by adjusting the theta arguments. For that, we need to take the partial derivatives of the cost function with respect to each theta parameter we have (if we have 4 dimensions then 4 theta + theta0), so we can obtain a smaller error by derivating the error. This equation can also be found in the internet very easily\n",
    "\n",
    "### - 5th Step: in the next step, as Andrew NG suggests, we need to update the theta parameters by substracting them a tiny number, in this case, the learning rate * each individual gradient, so in each iteration we'll have an updated value for these theta parameters in the right direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,1) (90,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-216-c9bbd4f4a344>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#linear equation formula\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mlinear_equation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtheta1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtheta2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtheta3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx3\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtheta4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#sigmoid function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,1) (90,1) "
     ]
    }
   ],
   "source": [
    "# in Object Oriented programming, this could be a method called fit, similar to what sklearn does\n",
    "\n",
    "epochs = 0\n",
    "cost_outputs = []\n",
    "\n",
    "while(epochs < 6000):\n",
    "    \n",
    "    #linear equation formula\n",
    "    linear_equation = theta0 + theta1 * x1 + theta2 * x2 + theta3 * x3 + theta4 * x4\n",
    "    \n",
    "    #sigmoid function\n",
    "    predictions = sigmoid(linear_equation)\n",
    "    \n",
    "    # I compute the cost function value just to plot it and see if this is decreasing.\n",
    "    cost_function = (- np.dot(np.transpose(y_train),np.log(predictions)) - np.dot(np.transpose(1-y_train),np.log(1-predictions)))/m\n",
    "    \n",
    "    \n",
    "    theta_0_partial_der = np.dot(np.ones((1,m)),predictions-y_train)/m\n",
    "    theta_1_partial_der = np.dot(np.transpose(x1),predictions-y_train)/m\n",
    "    theta_2_partial_der = np.dot(np.transpose(x2),predictions-y_train)/m\n",
    "    theta_3_partial_der = np.dot(np.transpose(x3),predictions-y_train)/m\n",
    "    theta_4_partial_der = np.dot(np.transpose(x4),predictions-y_train)/m\n",
    "    \n",
    "    # bellow here we update each theta value before ending the iteration so we can start\n",
    "    # the next one with a different value for each theta argument.\n",
    "    \n",
    "    theta0 = theta0 - learning_rate * theta_0_partial_der\n",
    "    theta1 = theta1 - learning_rate * theta_1_partial_der\n",
    "    theta2 = theta2 - learning_rate * theta_2_partial_der\n",
    "    theta3 = theta3 - learning_rate * theta_3_partial_der\n",
    "    theta4 = theta4 - learning_rate * theta_4_partial_der\n",
    "    \n",
    "    cost_outputs.append(cost_function)\n",
    "    epochs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cost_func = np.array(cost_outputs)\n",
    "cost_func = cost_func.reshape(6000,1)\n",
    "plt.plot(range(len(cost_func)),cost_func)\n",
    "plt.title(\"cost function\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# in OOP, this could be a method called predict, that returns an array of predited values,\n",
    "# similar to what sklearn does\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "test_x_1 = X_test[:,0]\n",
    "test_x_2 = X_test[:,1]\n",
    "test_x_3 = X_test[:,2]\n",
    "test_x_4 = X_test[:,3]\n",
    "\n",
    "test_x_1 = np.array(test_x_1)\n",
    "test_x_2 = np.array(test_x_2)\n",
    "test_x_3 = np.array(test_x_3)\n",
    "test_x_4 = np.array(test_x_4)\n",
    "\n",
    "test_x_1 = test_x_1.reshape(10,1)\n",
    "test_x_2 = test_x_2.reshape(10,1)\n",
    "test_x_3 = test_x_3.reshape(10,1)\n",
    "test_x_4 = test_x_4.reshape(10,1)\n",
    "\n",
    "index = list(range(10,90))    \n",
    "\n",
    "theta0 = theta0[:10]\n",
    "theta1 = theta1[:10]\n",
    "theta2 = theta2[:10]\n",
    "theta3 = theta3[:10]\n",
    "theta4 = theta4[:10]\n",
    "\n",
    "theta0 = theta0.reshape(10,1)\n",
    "theta1 = theta1.reshape(10,1)\n",
    "theta2 = theta2.reshape(10,1)\n",
    "theta3 = theta3.reshape(10,1)\n",
    "theta4 = theta4.reshape(10,1)\n",
    "\n",
    "linear_eq = theta0 + theta1 * test_x_1 + theta2 * test_x_2 + theta3 * test_x_3 + theta4 * test_x_4\n",
    "predictions = sigmoid(linear_eq)\n",
    "\n",
    "array_predictions =[]\n",
    "\n",
    "for prediction in predictions:\n",
    "    if(prediction >= 0.5):\n",
    "        array_predictions.append(1)\n",
    "    else:\n",
    "        array_predictions.append(0)\n",
    "\n",
    "print(accuracy_score(y_test, array_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
